Start of next file 
File: data_processor.py

import requests
from bs4 import BeautifulSoup
import io, base64
from pypdf import PdfReader
from youtube_transcript_api import YouTubeTranscriptApi


def extract_transcript(video_id):
    video_id = video_id[video_id.find('=')+1:]
    print(video_id)
    text = ""
    val = YouTubeTranscriptApi.get_transcript(video_id)
    for i in val:
        text += i["text"]
    return text

def extract_data_from_pdf(base64_pdf_string):
    pdf_file = base64.b64decode(base64_pdf_string)
    file = io.BytesIO(pdf_file)
    reader = PdfReader(file)
    text = ""
    for page in reader.pages:
        text_temp = page.extract_text()
        text += text_temp
    return text

def extract_text_from_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    text = soup.get_text()
    return text

Start of next file 
File: llm_processor.py

from dotenv import load_dotenv
import os
from langchain_core.prompts import ChatPromptTemplate
load_dotenv()
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser



OPEN_AI_KEY = os.getenv("OPEN_AI_KEY")

class prompts():
    def _init_(self) -> None:
        pass

    def notes_maker_prompt(self):
        prompt = ChatPromptTemplate.from_messages([
                ("system", "You are an expert notes compiler and summarizer"),
                ("user", """You will be provided with some data and you are to remove uneccesary data and summarize the data and make it look meaningful. Here is the data: 
                {input}
                 
                PROVIDE THE SUMMARIZED MEANINGFUL NOTES, ANYTHING ELSE IS NOT REQUIRED""")
            ])
        return prompt
    def process_recieved_notes(self):
        prompt = ChatPromptTemplate.from_messages([
                ("system", "You are an expert teacher who corrects notes and data"),
                ("user", """You will be provided with a paragraph which lacks some context and might not seem continous. You are to correct that data by rearranging the sentences meaningfully, in the end the paragraph should be completely. Remove sentence which you feel are unwanted: 
                {input}
                 
                PROVIDE MEANINGFUL NOTES, ANYTHING ELSE IS NOT REQUIRED""")
            ])
        return prompt
    

class llm_invoker:
    def _init_(self) -> None:
        self.prompts = prompts()
        self.llm =  ChatOpenAI(api_key=OPEN_AI_KEY)
        self.output_parser = StrOutputParser()
    
    def process_chunks(self, chunk_data):
        chain = self.prompts.notes_maker_prompt() | self.llm | self.output_parser
        processed_data = chain.invoke({"input": chunk_data})
        return processed_data
    
    def process_notes(self, notes_data):
        chain = self.prompts.process_recieved_notes() | self.llm | self.output_parser
        data = chain.invoke({"input": notes_data})
        return data

Start of next file 
File: notesmaker.py

import streamlit as st
import pdfplumber, base64
from data_processor import extract_data_from_pdf, extract_transcript, extract_text_from_url
from graph_retrieval_system import GraphRAG
from llm_processor import llm_invoker
from datetime import time

def main():
    st.set_page_config(page_title="AI Notes Maker", page_icon=":notebook_with_decorative_cover:")
    st.markdown("<style>body { background-color: black; color: white; }</style>", unsafe_allow_html=True)
    st.title("AI Notes Maker")

    content_type = st.selectbox("Select content type", ["PDF", "YouTube Link", "Website", "PowerPoint"])

    if content_type in ["YouTube Link", "Website"]:
        link = st.text_input("Enter the link")
    else:
        file = st.file_uploader(f"Upload a {content_type.lower()} file")

    if st.button("Submit"):
        if content_type == "PDF":
            if file is not None:
                bytes_data = file.getvalue()
                file2 = base64.b64encode(bytes_data)
                notes_data = extract_data_from_pdf(file2)
            else:
                st.warning("Please upload a PDF file.")
        elif content_type == "YouTube Link":
            if link:
                notes_data = extract_transcript(link)
            else:
                st.warning("Please enter a YouTube link.")
        elif content_type == "Website":
            if link:
                st.markdown(f'<iframe src="{link}" width="800" height="600"></iframe>', unsafe_allow_html=True)
                notes_data = extract_text_from_url(link)
            else:
                st.warning("Please enter a website link.")

        if notes_data:
            graph_rag = GraphRAG()
            llm = llm_invoker()
            graph_rag.constructGraph(notes_data)
            print("hello")
            sumamrized_data = []
            print("check 123")
            print(len(graph_rag.lines))
            i = 0
            while(i < len(len(graph_rag.lines)-1)):
                print("helllooo")
                temp_data = llm.process_chunks(graph_rag.lines[i] + graph_rag.lines[i+1]) 
                print(temp_data)
                sumamrized_data.append(temp_data)
            for i in range(0,len(graph_rag.lines)-1,2):
                print("helllooo")
                temp_data = llm.process_chunks(graph_rag.lines[i] + graph_rag.lines[i+1]) 
                print(temp_data)
                sumamrized_data.append(temp_data)
            
            final_notes = ""
            for i in range(0, len(sumamrized_data)-2, 3):
                print("helllooooo")
                sentence = sumamrized_data[i]+" "+ sumamrized_data[i+1]+" "+ sumamrized_data[i+2]
                temp_data = llm.process_notes(sentence)
                print(len(sentence))
                print("____")
                print(sentence)
                final_notes += temp_data
                time.sleep(21)

            st.write(final_notes)

                


if _name_ == "_main_":
    main()

